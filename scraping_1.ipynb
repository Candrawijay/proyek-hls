import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
os.listdir()

from google.colab import files

base_url = "https://sinta.kemdikbud.go.id/affiliations/profile/398/?page="
view_param = "&view=scopus"
start_page = 1
end_page = 448
data_list = []

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}

for page in range(start_page, end_page + 1):
    url = base_url + str(page) + view_param
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")


        articles = soup.find_all("div", class_="ar-list-item mb-5")

        for article in articles:
            title = article.find("a").text.strip() if article.find("a") else "No Title"
            link = article.find("a")["href"] if article.find("a") else "No Link"
            authors = article.find("div", class_="authors").text.strip() if article.find("div", class_="authors") else "Unknown"
            citations = article.find("span", class_="citation").text.strip() if article.find("span", class_="citation") else "0"

            data_list.append([title, link, authors, citations])

        print(f"Scraped page {page}")
        time.sleep(2)

df = pd.DataFrame(data_list, columns=["Title", "Link", "Authors", "Citations"])
df.to_csv("sinta_unila.csv", index=False)

print("Scraping selesai, dataÂ disimpan!")

files.download("sinta_unila.csv")
